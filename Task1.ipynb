{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Uni-Word Tokenizer--------------------------------------------------------------\n",
      "\n",
      "['This', 'is', 'not', 'a', 'test', 'anymore', '.', 'SpaceX', 'is', 'looking', 'to', 'make', 'International', 'Space', 'Station', 'astronaut', 'transfers', 'a', 'normal', 'part', 'of', 'NASA', 'operations', 'with', 'the', 'Crew-1', 'mission', '--', 'its', 'first', 'crew', 'rotation', 'flight', '--', 'this', 'Sunday', ',', 'Nov.', '15', '.', 'The', 'launch', 'got', 'pushed', 'back', 'from', 'Saturday', 'due', 'to', 'onshore', 'winds', 'and', 'potential', 'problems', 'with', 'recovery', 'operations', '.', 'SpaceX', \"'s\", 'groundbreaking', 'Demo-2', 'mission', 'delivered', 'two', 'NASA', 'astronauts', 'safely', 'to', 'the', 'ISS', 'in', 'May', '.', 'It', 'was', 'both', 'harrowing', 'and', 'exciting', 'as', 'actual', 'humans', 'tested', 'out', 'Crew', 'Dragon', 'for', 'the', 'first', 'time', '.', 'Crew-1', 'will', 'follow', 'in', 'the', 'footsteps', 'of', 'that', 'successful', 'mission', 'with', 'a', 'launch', 'on', 'a', 'Falcon', '9', 'rocket', 'from', 'Kennedy', 'Space', 'Center', 'in', 'Florida', '.', '.']\n",
      "------------------------------------------------Multiwordexpression Tokenizer------------------------------------------\n",
      "\n",
      "['This', 'is', 'not', 'a', 'test', 'anymore.', 'SpaceX', 'is', 'looking', 'to', 'make', 'International_Space_Station', 'astronaut', 'transfers', 'a', 'normal', 'part', 'of', 'NASA', 'operations', 'with', 'the', 'Crew-1_mission', '--', 'its', 'first', 'crew', 'rotation', 'flight', '--', 'this', 'Sunday,', 'Nov.', '15.', 'The', 'launch', 'got', 'pushed', 'back', 'from', 'Saturday', 'due', 'to', 'onshore', 'winds', 'and', 'potential', 'problems', 'with', 'recovery', 'operations.', \"SpaceX's\", 'groundbreaking', 'Demo-2', 'mission', 'delivered', 'two', 'NASA', 'astronauts', 'safely', 'to', 'the', 'ISS', 'in', 'May.', 'It', 'was', 'both', 'harrowing', 'and', 'exciting', 'as', 'actual', 'humans', 'tested', 'out', 'Crew_Dragon', 'for', 'the', 'first', 'time.', 'Crew-1', 'will', 'follow', 'in', 'the', 'footsteps', 'of', 'that', 'successful', 'mission', 'with', 'a', 'launch', 'on', 'a', 'Falcon_9', 'rocket', 'from', 'Kennedy_Space_Center', 'in', 'Florida.', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"--------------------------Uni-Word Tokenizer--------------------------------------------------------------\\n\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text= \"\"\"This is not a test anymore. \n",
    "SpaceX is looking to make International Space Station astronaut \n",
    "transfers a normal part of NASA operations with the Crew-1 mission -- \n",
    "its first crew rotation flight -- this Sunday, Nov. 15. \n",
    "The launch got pushed back from Saturday due to onshore winds and potential problems with recovery operations.\n",
    "\n",
    "SpaceX's groundbreaking Demo-2 mission delivered two NASA astronauts safely to the ISS in May. \n",
    "It was both harrowing and exciting as actual humans tested out Crew Dragon for the first time. \n",
    "Crew-1 will follow in the footsteps of that successful mission with a launch on a Falcon 9 rocket from Kennedy Space Center in Florida.\n",
    ".\"\"\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print(\"------------------------------------------------Multiwordexpression Tokenizer------------------------------------------\\n\")\n",
    "\n",
    "#multiwordexpression Tokenizer\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer =MWETokenizer([('International' , 'Space','Station'),('Crew', 'Dragon'),('Falcon','9'),('Kennedy','Space','Center'),])\n",
    "tokenizer.add_mwe(('Crew-1','mission'))\n",
    "mwe_token=tokenizer.tokenize(text.split())\n",
    "print(mwe_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats ----------- cat\n",
      "lords ----------- lord\n",
      "dogs ----------- dog\n",
      "flying ----------- fly\n",
      "smiling ----------- smile\n",
      "driving ----------- drive\n",
      "died ----------- die\n",
      "tried ----------- try\n",
      "feeling ----------- feel\n",
      "['The', 'coronavirus', 'pandemic', 'is', 'expected', 'to', 'get', 'much', 'worse', 'during', 'the', 'winter', ',', 'as', 'the', 'number', 'of', 'deaths', 'and', 'infection', 'cases', 'continue', 'to', 'rise', '.', 'While', 'measures', 'like', 'wearing', 'masks', 'and', 'social', 'distancing', 'can', 'help', 'reduce', 'the', 'amount', 'of', 'cases', ',', 'researchers', 'are', 'racing', 'to', 'create', 'a', 'vaccine', 'that', 'would', 'give', 'a', 'more', 'permanent', 'solution', 'to', 'the', 'pandemic', '.']\n",
      "The coronavirus pandemic be expect to get much worse during the winter , as the number of deaths and infection case continue to rise . While measure like wear mask and social distance can help reduce the amount of case , researchers be race to create a vaccine that would give a more permanent solution to the pandemic .\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Create WordNetLemmatizer object \n",
    "wnl = WordNetLemmatizer() \n",
    "\n",
    "# single word lemmatization examples \n",
    "list1 = ['cats', 'lords', 'dogs', 'flying', 'smiling',\n",
    "         'driving', 'died', 'tried','feeling'] \n",
    "\n",
    "for words in list1: \n",
    "    print(words +\" ----------- \"  + wnl.lemmatize(words,'v'))\n",
    "\n",
    "    \n",
    "    \n",
    "string = \"\"\"The coronavirus pandemic is expected to get much worse during the winter, \n",
    "as the number of deaths and infection cases continue to rise. \n",
    "While measures like wearing masks and social distancing can help reduce the amount of cases, researchers are racing to \n",
    "create a vaccine that would give a more permanent solution to the pandemic. \"\"\"\n",
    "\n",
    "\n",
    "# Converting String into tokens \n",
    "list2 = nltk.word_tokenize(string) \n",
    "print(list2) \n",
    " \n",
    "\n",
    "lemmatized_string = ' '.join([wnl.lemmatize(words,'v') for words in list2]) \n",
    "\n",
    "print(lemmatized_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
